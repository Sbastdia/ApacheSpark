{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da0af853-c0da-437f-9589-93e77c0c71ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "780edd96-eaa8-4118-bbc5-3096dd9b87b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "equipos=pd.read_csv('equipo.csv')\n",
    "partidos= pd.read_csv('partidos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "941e141f-a102-4ac0-92dd-d26d698e2a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ajax</th>\n",
       "      <th>ajax.png</th>\n",
       "      <th>Paises Bajos</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Atalanta</td>\n",
       "      <td>atalanta.png</td>\n",
       "      <td>Italia</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Atlético</td>\n",
       "      <td>atletico.png</td>\n",
       "      <td>España</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barcelona</td>\n",
       "      <td>barcelona.png</td>\n",
       "      <td>España</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bayern</td>\n",
       "      <td>bayern.png</td>\n",
       "      <td>Alemania</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Benfica</td>\n",
       "      <td>benfica.png</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Ajax       ajax.png Paises Bajos  14\n",
       "0   Atalanta   atalanta.png       Italia   6\n",
       "1   Atlético   atletico.png       España  11\n",
       "2  Barcelona  barcelona.png       España  17\n",
       "3     Bayern     bayern.png     Alemania  16\n",
       "4    Benfica    benfica.png     Portugal  15"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equipos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fabcac3-728a-4596-9b08-8779b7a0e0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>14/09/2021 16:45</th>\n",
       "      <th>29</th>\n",
       "      <th>2</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>33.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14/09/2021 16:45</td>\n",
       "      <td>31</td>\n",
       "      <td>19</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14/09/2021 16:45</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14/09/2021 19:00</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14/09/2021 19:00</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14/09/2021 19:00</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   14/09/2021 16:45  29   2  33  34  33.1\n",
       "0  14/09/2021 16:45  31  19  33  34    33\n",
       "1  14/09/2021 16:45  25  24  33  34    33\n",
       "2  14/09/2021 19:00  11   6  33  34    33\n",
       "3  14/09/2021 19:00   4   5  33  34    33\n",
       "4  14/09/2021 19:00  16  30  33  34    33"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partidos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "919cf476-8eb7-4e71-b6ec-83b6387992cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Equipo</th>\n",
       "      <th>Imagen</th>\n",
       "      <th>Pais</th>\n",
       "      <th>Indice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Atalanta</td>\n",
       "      <td>atalanta.png</td>\n",
       "      <td>Italia</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Atlético</td>\n",
       "      <td>atletico.png</td>\n",
       "      <td>España</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barcelona</td>\n",
       "      <td>barcelona.png</td>\n",
       "      <td>España</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bayern</td>\n",
       "      <td>bayern.png</td>\n",
       "      <td>Alemania</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Benfica</td>\n",
       "      <td>benfica.png</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Equipo         Imagen      Pais  Indice\n",
       "0   Atalanta   atalanta.png    Italia       6\n",
       "1   Atlético   atletico.png    España      11\n",
       "2  Barcelona  barcelona.png    España      17\n",
       "3     Bayern     bayern.png  Alemania      16\n",
       "4    Benfica    benfica.png  Portugal      15"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equipos.columns=['Equipo', 'Imagen', 'Pais', 'Indice']\n",
    "equipos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eec68638-6bd3-4567-8cfe-ba7d3c779edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Local</th>\n",
       "      <th>Visitante</th>\n",
       "      <th>Prob_local</th>\n",
       "      <th>Prob_empate</th>\n",
       "      <th>Prob_visitante</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14/09/2021 16:45</td>\n",
       "      <td>31</td>\n",
       "      <td>19</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14/09/2021 16:45</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14/09/2021 19:00</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14/09/2021 19:00</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14/09/2021 19:00</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Fecha  Local  Visitante  Prob_local  Prob_empate  Prob_visitante\n",
       "0  14/09/2021 16:45     31         19          33           34              33\n",
       "1  14/09/2021 16:45     25         24          33           34              33\n",
       "2  14/09/2021 19:00     11          6          33           34              33\n",
       "3  14/09/2021 19:00      4          5          33           34              33\n",
       "4  14/09/2021 19:00     16         30          33           34              33"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partidos.columns=['Fecha', 'Local', 'Visitante', 'Prob_local', 'Prob_empate', 'Prob_visitante']\n",
    "partidos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d3d878d-97d0-4f78-b3c2-dde90f6ee347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "722a1119-2c00-4b84-a210-09859b31d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7c9f69d-0eac-459c-a804-6d3b7d1948dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/18 11:22:57 WARN Utils: Your hostname, MacBook-Air-de-Sara.local resolves to a loopback address: 127.0.0.1; using 192.168.1.130 instead (on interface en0)\n",
      "23/02/18 11:22:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/sara/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/02/18 11:22:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/18 11:22:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddf675cf-f2eb-480e-83e4-94e86bd0ba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=spark.createDataFrame(equipos)\n",
    "df2=spark.createDataFrame(partidos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47ea1e7d-288f-4b4e-a242-c0c61c78d053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+----------+------+\n",
      "|        Equipo|            Imagen|      Pais|Indice|\n",
      "+--------------+------------------+----------+------+\n",
      "|      Atalanta|      atalanta.png|    Italia|     6|\n",
      "|      Atlético|      atletico.png|    España|    11|\n",
      "|     Barcelona|     barcelona.png|    España|    17|\n",
      "|        Bayern|        bayern.png|  Alemania|    16|\n",
      "|       Benfica|       benfica.png|  Portugal|    15|\n",
      "|      Besiktas|      besiktas.png|   Turquía|     2|\n",
      "|       Chelsea|       chelsea.png|Inglaterra|     1|\n",
      "|   Club Brugge|   club_brugge.png|   Bélgica|    26|\n",
      "|      Dortmund|      dortmund.png|  Alemania|    27|\n",
      "|   Dynamo Kyiv|   dynamo_kyiv.png|   Ucrania|    28|\n",
      "|Internazionale|internazionale.png|    Italia|    29|\n",
      "|      Juventus|      juventus.png|    Italia|     9|\n",
      "|       Leipzig|       leipzig.png|  Alemania|     8|\n",
      "|     Liverpool|     liverpool.png|Inglaterra|    24|\n",
      "|          LOSC|          losc.png|   Francia|    23|\n",
      "|         Malmö|         malmo.png|    Suecia|    22|\n",
      "|     Man. City|      man_city.png|Inglaterra|    21|\n",
      "|   Man. United|    man_united.png|Inglaterra|    20|\n",
      "|         Milan|         milan.png|    Italia|    19|\n",
      "|         Paris|         paris.png|   Francia|    14|\n",
      "+--------------+------------------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9de11fa2-92a8-4adb-975f-05cc6a4c8069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "|           Fecha|Local|Visitante|Prob_local|Prob_empate|Prob_visitante|\n",
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "|14/09/2021 16:45|   31|       19|        33|         34|            33|\n",
      "|14/09/2021 16:45|   25|       24|        33|         34|            33|\n",
      "|14/09/2021 19:00|   11|        6|        33|         34|            33|\n",
      "|14/09/2021 19:00|    4|        5|        33|         34|            33|\n",
      "|14/09/2021 19:00|   16|       30|        33|         34|            33|\n",
      "|14/09/2021 19:00|   17|       13|        33|         34|            33|\n",
      "|14/09/2021 19:00|    8|       32|        33|         34|            33|\n",
      "|15/09/2021 16:45|    7|       10|        33|         34|            33|\n",
      "|15/09/2021 16:45|   27|       26|        33|         34|            33|\n",
      "|15/09/2021 19:00|    9|       21|        33|         34|            33|\n",
      "|15/09/2021 19:00|   18|       14|        33|         34|            33|\n",
      "|15/09/2021 19:00|    3|       22|        33|         34|            33|\n",
      "|15/09/2021 19:00|   15|       20|        33|         34|            33|\n",
      "|15/09/2021 19:00|   28|        1|        33|         34|            33|\n",
      "|15/09/2021 19:00|   12|       23|        33|         34|            33|\n",
      "|28/09/2021 16:45|    1|        7|        33|         34|            33|\n",
      "|28/09/2021 16:45|   26|       12|        33|         34|            33|\n",
      "|28/09/2021 19:00|   21|       18|        33|         34|            33|\n",
      "|28/09/2021 19:00|   14|        9|        33|         34|            33|\n",
      "|28/09/2021 19:00|   22|       15|        33|         34|            33|\n",
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ad4188-e7aa-4bf8-9056-72ec9ed56696",
   "metadata": {},
   "source": [
    "Operaciones básicas en DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a2e260b-2e80-4abc-b1bd-f05cc7c86b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c1ae35d-929b-479e-94db-f8c325c95ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29f5e399-87aa-4bca-8523-23377a44eef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Equipo', 'Imagen', 'Pais', 'Indice']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "777cf9ea-729e-4f68-a2a4-d89b6ef62ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fecha', 'Local', 'Visitante', 'Prob_local', 'Prob_empate', 'Prob_visitante']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ffe7611-b36b-44dc-8777-80bf0c5fe376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Equipo', 'string'),\n",
       " ('Imagen', 'string'),\n",
       " ('Pais', 'string'),\n",
       " ('Indice', 'bigint')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5522c0ec-0e46-4abc-b526-4d4ad663b574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fecha', 'string'),\n",
       " ('Local', 'bigint'),\n",
       " ('Visitante', 'bigint'),\n",
       " ('Prob_local', 'bigint'),\n",
       " ('Prob_empate', 'bigint'),\n",
       " ('Prob_visitante', 'bigint')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69113073-e23c-4757-8e9a-0be3d4acdbbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Equipo,StringType,true),StructField(Imagen,StringType,true),StructField(Pais,StringType,true),StructField(Indice,LongType,true)))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d96b135b-51d3-4a96-ad95-8988d08d0b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Fecha,StringType,true),StructField(Local,LongType,true),StructField(Visitante,LongType,true),StructField(Prob_local,LongType,true),StructField(Prob_empate,LongType,true),StructField(Prob_visitante,LongType,true)))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90f7aafd-5087-439c-b813-668194b5eed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Equipo: string (nullable = true)\n",
      " |-- Imagen: string (nullable = true)\n",
      " |-- Pais: string (nullable = true)\n",
      " |-- Indice: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a192cd3-74fc-4937-80d0-e9927b33edb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Fecha: string (nullable = true)\n",
      " |-- Local: long (nullable = true)\n",
      " |-- Visitante: long (nullable = true)\n",
      " |-- Prob_local: long (nullable = true)\n",
      " |-- Prob_empate: long (nullable = true)\n",
      " |-- Prob_visitante: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cb09d24-ebe6-4727-b28d-6edf8e622c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|        Equipo|      Pais|\n",
      "+--------------+----------+\n",
      "|      Atalanta|    Italia|\n",
      "|      Atlético|    España|\n",
      "|     Barcelona|    España|\n",
      "|        Bayern|  Alemania|\n",
      "|       Benfica|  Portugal|\n",
      "|      Besiktas|   Turquía|\n",
      "|       Chelsea|Inglaterra|\n",
      "|   Club Brugge|   Bélgica|\n",
      "|      Dortmund|  Alemania|\n",
      "|   Dynamo Kyiv|   Ucrania|\n",
      "|Internazionale|    Italia|\n",
      "|      Juventus|    Italia|\n",
      "|       Leipzig|  Alemania|\n",
      "|     Liverpool|Inglaterra|\n",
      "|          LOSC|   Francia|\n",
      "|         Malmö|    Suecia|\n",
      "|     Man. City|Inglaterra|\n",
      "|   Man. United|Inglaterra|\n",
      "|         Milan|    Italia|\n",
      "|         Paris|   Francia|\n",
      "+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select('Equipo', 'Pais').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73ddfae4-edfc-4c2a-9316-e7d05e411e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|Local|Visitante|\n",
      "+-----+---------+\n",
      "|   31|       19|\n",
      "|   25|       24|\n",
      "|   11|        6|\n",
      "|    4|        5|\n",
      "|   16|       30|\n",
      "|   17|       13|\n",
      "|    8|       32|\n",
      "|    7|       10|\n",
      "|   27|       26|\n",
      "|    9|       21|\n",
      "|   18|       14|\n",
      "|    3|       22|\n",
      "|   15|       20|\n",
      "|   28|        1|\n",
      "|   12|       23|\n",
      "|    1|        7|\n",
      "|   26|       12|\n",
      "|   21|       18|\n",
      "|   14|        9|\n",
      "|   22|       15|\n",
      "+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select('Local', 'Visitante').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1395e6f1-362c-4335-83db-8981644bf9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+------+------+\n",
      "|        Equipo|            Imagen|  Pais|Indice|\n",
      "+--------------+------------------+------+------+\n",
      "|      Atalanta|      atalanta.png|Italia|     6|\n",
      "|Internazionale|internazionale.png|Italia|    29|\n",
      "|      Juventus|      juventus.png|Italia|     9|\n",
      "|         Milan|         milan.png|Italia|    19|\n",
      "+--------------+------------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(df1[\"Pais\"] == 'Italia').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ec9b79f-5f32-4cef-81e2-abdc86adb9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "|           Fecha|Local|Visitante|Prob_local|Prob_empate|Prob_visitante|\n",
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "|14/09/2021 16:45|   25|       24|        33|         34|            33|\n",
      "|02/11/2021 20:00|   25|       16|        33|         34|            33|\n",
      "|23/11/2021 20:00|   25|       30|        33|         34|            33|\n",
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.filter(df2[\"Local\"] == 25).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9ff2e14-1c85-4cc4-8788-ae202d124c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+------+------+\n",
      "|        Equipo|            Imagen|  Pais|Indice|\n",
      "+--------------+------------------+------+------+\n",
      "|      Atalanta|      atalanta.png|Italia|     6|\n",
      "|Internazionale|internazionale.png|Italia|    29|\n",
      "|      Juventus|      juventus.png|Italia|     9|\n",
      "|         Milan|         milan.png|Italia|    19|\n",
      "+--------------+------------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(col(\"Pais\") == 'Italia').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc9b6381-5978-4e9c-bbd1-9825d32457c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "|           Fecha|Local|Visitante|Prob_local|Prob_empate|Prob_visitante|\n",
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "|14/09/2021 16:45|   25|       24|        33|         34|            33|\n",
      "|02/11/2021 20:00|   25|       16|        33|         34|            33|\n",
      "|23/11/2021 20:00|   25|       30|        33|         34|            33|\n",
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.filter(col(\"Local\") == 25).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13d81401-73b5-45f5-a960-a0c09135b47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+----------+------+\n",
      "|    Equipo|        Imagen|      Pais|Indice|\n",
      "+----------+--------------+----------+------+\n",
      "|   Chelsea|   chelsea.png|Inglaterra|     1|\n",
      "|  Besiktas|  besiktas.png|   Turquía|     2|\n",
      "| Wolfsburg| wolfsburg.png|  Alemania|     4|\n",
      "|  Atalanta|  atalanta.png|    Italia|     6|\n",
      "|Young Boys|young_boys.png|     Suiza|     7|\n",
      "+----------+--------------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.sort(\"Indice\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acb9e57c-2417-4cde-9909-c63834eb007a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "|           Fecha|Local|Visitante|Prob_local|Prob_empate|Prob_visitante|\n",
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "|19/10/2021 19:00|    1|       10|        33|         34|            33|\n",
      "|07/12/2021 20:00|    1|       28|        33|         34|            33|\n",
      "|28/09/2021 16:45|    1|        7|        33|         34|            33|\n",
      "|08/12/2021 20:00|    2|       29|        33|         34|            33|\n",
      "|02/11/2021 20:00|    2|       19|        33|         34|            33|\n",
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.sort(\"Local\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a65c109c-da99-4e9d-8130-f0f484e82953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "|           Fecha|Local|Visitante|Prob_local|Prob_empate|Prob_visitante|\n",
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "|24/11/2021 17:45|    7|        1|        33|         34|            33|\n",
      "|03/11/2021 20:00|   10|        1|        33|         34|            33|\n",
      "|15/09/2021 19:00|   28|        1|        33|         34|            33|\n",
      "|20/10/2021 19:00|   19|        2|        33|         34|            33|\n",
      "|23/11/2021 20:00|   31|        2|        33|         34|            33|\n",
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.sort(\"Visitante\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0ac7f48-d785-4c4a-9112-8397bab3fe2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+--------+------+\n",
      "|        Equipo|            Imagen|    Pais|Indice|\n",
      "+--------------+------------------+--------+------+\n",
      "|    Villarreal|    villarreal.png|  España|    30|\n",
      "|Internazionale|internazionale.png|  Italia|    29|\n",
      "|   Dynamo Kyiv|   dynamo_kyiv.png| Ucrania|    28|\n",
      "|      Dortmund|      dortmund.png|Alemania|    27|\n",
      "|   Club Brugge|   club_brugge.png| Bélgica|    26|\n",
      "+--------------+------------------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the data in descending order.\n",
    "df1.sort(desc(\"Indice\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b2591f8-93f8-4f66-b644-fb94015abc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "|           Fecha|Local|Visitante|Prob_local|Prob_empate|Prob_visitante|\n",
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "|29/09/2021 16:45|   32|       17|        33|         34|            33|\n",
      "|20/10/2021 19:00|   32|       13|        33|         34|            33|\n",
      "|08/12/2021 17:45|   32|        8|        33|         34|            33|\n",
      "|23/11/2021 20:00|   31|        2|        33|         34|            33|\n",
      "|14/09/2021 16:45|   31|       19|        33|         34|            33|\n",
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.sort(desc(\"Local\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79f27095-404b-4664-9bb6-9e6d1650ab85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "|           Fecha|Local|Visitante|Prob_local|Prob_empate|Prob_visitante|\n",
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "|02/11/2021 20:00|   13|       32|        33|         34|            33|\n",
      "|23/11/2021 20:00|   17|       32|        33|         34|            33|\n",
      "|14/09/2021 19:00|    8|       32|        33|         34|            33|\n",
      "|08/12/2021 20:00|   19|       31|        33|         34|            33|\n",
      "|29/09/2021 16:45|    2|       31|        33|         34|            33|\n",
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.sort(desc(\"Visitante\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191c9066-0613-47d7-bab7-8fb16138a6e1",
   "metadata": {},
   "source": [
    "### Consultas SQL\n",
    "* Ejecución de consultas tipo SQL.\n",
    "* También podemos realizar análisis de datos escribiendo consultas similares a SQL. Para realizar consultas similares a SQL, necesitamos registrar el DataFrame como una Vista temporal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b0a17f1-63cd-457d-a276-73a0687b0074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+-------+------+\n",
      "|  Equipo|      Imagen|   Pais|Indice|\n",
      "+--------+------------+-------+------+\n",
      "|Besiktas|besiktas.png|Turquía|     2|\n",
      "+--------+------------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register DataFrame as Temporary Table\n",
    "df1.createOrReplaceTempView(\"temp_table1\")\n",
    "\n",
    "# Execute SQL-Like query.\n",
    "spark.sql(\"select * from temp_table1 where Indice = 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1a5e8db-9e20-4d45-8a5e-92c8cfc5b139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "|           Fecha|Local|Visitante|Prob_local|Prob_empate|Prob_visitante|\n",
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "|14/09/2021 16:45|   25|       24|        33|         34|            33|\n",
      "|02/11/2021 20:00|   25|       16|        33|         34|            33|\n",
      "|23/11/2021 20:00|   25|       30|        33|         34|            33|\n",
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register DataFrame as Temporary Table\n",
    "df2.createOrReplaceTempView(\"temp_table2\")\n",
    "\n",
    "# Execute SQL-Like query.\n",
    "spark.sql(\"select * from temp_table2 where Local = 25\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f1108c3-9607-4f8d-9bbe-7368297a179c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Indice|\n",
      "+------+\n",
      "|     6|\n",
      "|    17|\n",
      "|    11|\n",
      "|     2|\n",
      "|    15|\n",
      "|    16|\n",
      "|    26|\n",
      "|    29|\n",
      "|     9|\n",
      "|    27|\n",
      "+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct Indice from temp_table1\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79362d21-70d6-4d16-8b67-988b1e3c39b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Local|\n",
      "+-----+\n",
      "|    7|\n",
      "|   31|\n",
      "|   25|\n",
      "|    9|\n",
      "|   27|\n",
      "|   17|\n",
      "|    8|\n",
      "|   11|\n",
      "|    4|\n",
      "|   18|\n",
      "+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct Local from temp_table2\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21be0c82-f32b-4145-9d1f-b23535f7e96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+----------+------+\n",
      "|  Equipo|      Imagen|      Pais|Indice|\n",
      "+--------+------------+----------+------+\n",
      "|Atalanta|atalanta.png|    Italia|     6|\n",
      "|Atlético|atletico.png|    España|    11|\n",
      "|  Bayern|  bayern.png|  Alemania|    16|\n",
      "| Benfica| benfica.png|  Portugal|    15|\n",
      "|Besiktas|besiktas.png|   Turquía|     2|\n",
      "| Chelsea| chelsea.png|Inglaterra|     1|\n",
      "|Juventus|juventus.png|    Italia|     9|\n",
      "| Leipzig| leipzig.png|  Alemania|     8|\n",
      "|   Paris|   paris.png|   Francia|    14|\n",
      "|   Porto|   porto.png|  Portugal|    13|\n",
      "+--------+------------+----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from temp_table1 where Indice <=16\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23240ba2-c841-42c6-8a63-f8b32c245d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "|           Fecha|Local|Visitante|Prob_local|Prob_empate|Prob_visitante|\n",
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "|14/09/2021 19:00|   11|        6|        33|         34|            33|\n",
      "|14/09/2021 19:00|    4|        5|        33|         34|            33|\n",
      "|14/09/2021 19:00|   16|       30|        33|         34|            33|\n",
      "|14/09/2021 19:00|    8|       32|        33|         34|            33|\n",
      "|15/09/2021 16:45|    7|       10|        33|         34|            33|\n",
      "|15/09/2021 19:00|    9|       21|        33|         34|            33|\n",
      "|15/09/2021 19:00|    3|       22|        33|         34|            33|\n",
      "|15/09/2021 19:00|   15|       20|        33|         34|            33|\n",
      "|15/09/2021 19:00|   12|       23|        33|         34|            33|\n",
      "|28/09/2021 16:45|    1|        7|        33|         34|            33|\n",
      "+----------------+-----+---------+----------+-----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from temp_table2 where Local <=16\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb01040-1afd-4879-a6f1-11fd97dc2133",
   "metadata": {},
   "source": [
    "Si queremos consultar las bases de datos existentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9204b1d-072d-440b-8bf8-2ed25a4b40fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/Users/sara/Desktop/SARA/UAX/3º%20CURSO/Segundo%20cuatrimestre/Inteligencia%20artificial/ApacheSpark/spark-warehouse')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a634ce-157a-4300-8e2d-01794bef15ba",
   "metadata": {},
   "source": [
    "Guardamos DataFrame como tabla HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "379c9bca-3c2d-4c0c-a9a7-2cddca1dd51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/18 11:24:23 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/02/18 11:24:25 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n"
     ]
    }
   ],
   "source": [
    "# Save as HIVE tables.\n",
    "df1.write.saveAsTable(\"hive_equiposdf\", mode = \"overwrite\")\n",
    "df2.write.saveAsTable(\"hive_partidosdf\", mode = \"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abdb284-a38d-417a-a259-8cdaa41ba286",
   "metadata": {},
   "source": [
    "Podemos comprobar su estructura con SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6b1ff6f-5d0c-4058-ad93-3811e5072b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|  Equipo|   string|   null|\n",
      "|  Imagen|   string|   null|\n",
      "|    Pais|   string|   null|\n",
      "|  Indice|   bigint|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe table hive_equiposdf\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "24e3df97-fdf4-429d-a389-648746af5935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------+\n",
      "|      col_name|data_type|comment|\n",
      "+--------------+---------+-------+\n",
      "|         Fecha|   string|   null|\n",
      "|         Local|   bigint|   null|\n",
      "|     Visitante|   bigint|   null|\n",
      "|    Prob_local|   bigint|   null|\n",
      "|   Prob_empate|   bigint|   null|\n",
      "|Prob_visitante|   bigint|   null|\n",
      "+--------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe table hive_partidosdf\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4d504b-c663-4290-9698-98085f196d27",
   "metadata": {},
   "source": [
    "Si queremos crear una tabla no gestionada, también conocida como tabla externa, necesitamos indicar la ruta de los datos en el momento de creación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7732da7-e4a1-468c-a72f-7445497297ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/18 11:24:32 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n"
     ]
    }
   ],
   "source": [
    "df2.write.option(\"path\", \" /tmp/datos/partidos\").saveAsTable(\"partidosExterna\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6d0c63-691d-43f4-81a9-fa68e735277a",
   "metadata": {},
   "source": [
    "**CREAR UN DATAFRAME A PARTIR DE UN ARCHIVO CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95a76444-be98-48a3-bfc4-dd5c2a569aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=spark.read.csv(\"file:/Users/sara/Desktop/SARA/UAX/3º CURSO/Segundo cuatrimestre/Inteligencia artificial/ApacheSpark/equipo.csv\", sep=\"|\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "525e1458-656f-4761-a9fc-cadd8a31b382",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=spark.read.csv(\"file:/Users/sara/Desktop/SARA/UAX/3º CURSO/Segundo cuatrimestre/Inteligencia artificial/ApacheSpark/partidos.csv\", sep=\"|\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0809dcd4-59bd-49a4-b193-ce267562979c",
   "metadata": {},
   "source": [
    "### FUNDAMENTOS DE APACHE SPARK: FUNCIONES AVANZADAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "39f77996-8f0a-4459-b95a-6907b51acb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c379a544-daec-4d73-a798-a5d088f1e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=spark.createDataFrame(equipos)\n",
    "df2=spark.createDataFrame(partidos)\n",
    "# Create Temp Tables\n",
    "df1.createOrReplaceTempView(\"equipdf\")\n",
    "df2.createOrReplaceTempView(\"partidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe08a8c-c0d4-4925-8d21-32025949e326",
   "metadata": {},
   "source": [
    "* Podemos establecer el tamaño de la tabla de transmisión para que diga 50 MB de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4fe7c91d-6735-464e-a7d2-bdee17287198",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 50 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6424738e-b330-464a-b3eb-cccd74a6934e",
   "metadata": {},
   "source": [
    "* Podemos verificar el tamaño de la tabla de transmisión de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fc58c5c3-f522-4a4b-b7a2-d4f256bf21aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default size of broadcast table is 50.0 MB.\n"
     ]
    }
   ],
   "source": [
    "size = int(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")) / (1024 * 1024)\n",
    "print(\"Default size of broadcast table is {0} MB.\".format(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2318ffaf-d2ba-45f2-b520-211120d89517",
   "metadata": {},
   "source": [
    "### Almacenamiento en caché\n",
    "Podemos usar la función de caché / persistencia para mantener el marco de datos en la memoria. Puede mejorar significativamente el rendimiento de su aplicación Spark si almacenamos en caché los datos que necesitamos usar con mucha frecuencia en nuestra aplicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d7458e22-43ef-406d-81a9-00a4f83ad38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Used : True\n",
      "Disk Used : True\n"
     ]
    }
   ],
   "source": [
    "df1.cache()\n",
    "df1.count()\n",
    "print(\"Memory Used : {0}\".format(df1.storageLevel.useMemory))\n",
    "print(\"Disk Used : {0}\".format(df1.storageLevel.useDisk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c2577a84-71db-4aea-a1b0-6532f0aca562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Used : True\n",
      "Disk Used : True\n"
     ]
    }
   ],
   "source": [
    "df2.cache()\n",
    "df2.count()\n",
    "print(\"Memory Used : {0}\".format(df2.storageLevel.useMemory))\n",
    "print(\"Disk Used : {0}\".format(df2.storageLevel.useDisk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18149b2c-e3a3-4bff-aa2f-af66fcc88e72",
   "metadata": {},
   "source": [
    "Cuando usamos la función de caché, usará el nivel de almacenamiento como Memory_Only hasta Spark 2.0.2. Desde Spark 2.1.x es Memory_and_DISK.\n",
    "\n",
    "Sin embargo, si necesitamos especificar los distintos niveles de almacenamiento disponibles, podemos usar el método persist( ). Por ejemplo, si necesitamos mantener los datos solo en la memoria, podemos usar el siguiente fragmento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "80ee044a-ab73-4537-9256-72ab9de779be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b8ef4bcd-5ae6-4998-aaad-57e138cf4ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Used : True\n",
      "Disk Used : True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/18 11:40:36 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "df2.persist(StorageLevel.MEMORY_ONLY)\n",
    "df2.count()\n",
    "print(\"Memory Used : {0}\".format(df1.storageLevel.useMemory))\n",
    "print(\"Disk Used : {0}\".format(df1.storageLevel.useDisk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e5b532-2c1b-4a04-bdaf-2c6dcf044135",
   "metadata": {},
   "source": [
    "### No persistir\n",
    "También es importante eliminar la memoria caché de los datos cuando ya no sean necesarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "29d75f23-88cb-4d9b-a245-b5a852461a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Equipo: string, Imagen: string, Pais: string, Indice: bigint]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3e9bded1-bc6f-4301-b4ee-ef2f219b3b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Fecha: string, Local: bigint, Visitante: bigint, Prob_local: bigint, Prob_empate: bigint, Prob_visitante: bigint]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "77340bb0-2f77-470e-88db-2f7b20003de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sara/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc=spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2b7ce9b2-8f57-4426-9fda-440932ace325",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e73150-902d-4a1d-a7c1-3d01bc5c4562",
   "metadata": {},
   "source": [
    "**Expresiones SQL**\n",
    "\n",
    "También podemos usar la expresión SQL para la manipulación de datos. Tenemos la función **expr** y también una variante de un método de selección como **selectExpr** para la evaluación de expresiones SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e1abb405-5e74-4b1d-ae09-a72d7cc31401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+----------+------+------------+\n",
      "|        Equipo|            Imagen|      Pais|Indice|indice_level|\n",
      "+--------------+------------------+----------+------+------------+\n",
      "|      Atalanta|      atalanta.png|    Italia|     6| indice_bajo|\n",
      "|      Atlético|      atletico.png|    España|    11|indice_medio|\n",
      "|     Barcelona|     barcelona.png|    España|    17|indice_medio|\n",
      "|        Bayern|        bayern.png|  Alemania|    16|indice_medio|\n",
      "|       Benfica|       benfica.png|  Portugal|    15|indice_medio|\n",
      "|      Besiktas|      besiktas.png|   Turquía|     2| indice_bajo|\n",
      "|       Chelsea|       chelsea.png|Inglaterra|     1| indice_bajo|\n",
      "|   Club Brugge|   club_brugge.png|   Bélgica|    26| indice_alto|\n",
      "|      Dortmund|      dortmund.png|  Alemania|    27| indice_alto|\n",
      "|   Dynamo Kyiv|   dynamo_kyiv.png|   Ucrania|    28| indice_alto|\n",
      "|Internazionale|internazionale.png|    Italia|    29| indice_alto|\n",
      "|      Juventus|      juventus.png|    Italia|     9| indice_bajo|\n",
      "|       Leipzig|       leipzig.png|  Alemania|     8| indice_bajo|\n",
      "|     Liverpool|     liverpool.png|Inglaterra|    24| indice_alto|\n",
      "|          LOSC|          losc.png|   Francia|    23| indice_alto|\n",
      "|         Malmö|         malmo.png|    Suecia|    22| indice_alto|\n",
      "|     Man. City|      man_city.png|Inglaterra|    21| indice_alto|\n",
      "|   Man. United|    man_united.png|Inglaterra|    20| indice_alto|\n",
      "|         Milan|         milan.png|    Italia|    19|indice_medio|\n",
      "|         Paris|         paris.png|   Francia|    14|indice_medio|\n",
      "+--------------+------------------+----------+------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Intentemos categorizar los Indices en Bajo, Medio y Alto según la categorización a continuación.\n",
    "\n",
    "# <=10: indice_bajo\n",
    "# 10 - 20: indice_medio\n",
    "#>= 20: indice_alto\n",
    "\n",
    "cond = \"\"\"case when Indice <=10 then 'indice_bajo'\n",
    "               else case when Indice <20 then 'indice_medio'\n",
    "                    else case when Indice >=20 then 'indice_alto'\n",
    "                         else 'invalid_indice'\n",
    "                              end\n",
    "                         end\n",
    "                end as indice_level\"\"\"\n",
    "\n",
    "newdf1 = df1.withColumn(\"indice_level\", expr(cond))\n",
    "newdf1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a236e24-63da-45b0-ad96-2405c11545de",
   "metadata": {},
   "source": [
    "### Usando la función selectExpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1d6c2564-fca8-47cd-a515-58086a5c9533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+----------+------+------------+\n",
      "|        Equipo|            Imagen|      Pais|Indice|indice_level|\n",
      "+--------------+------------------+----------+------+------------+\n",
      "|      Atalanta|      atalanta.png|    Italia|     6| indice_bajo|\n",
      "|      Atlético|      atletico.png|    España|    11|indice_medio|\n",
      "|     Barcelona|     barcelona.png|    España|    17|indice_medio|\n",
      "|        Bayern|        bayern.png|  Alemania|    16|indice_medio|\n",
      "|       Benfica|       benfica.png|  Portugal|    15|indice_medio|\n",
      "|      Besiktas|      besiktas.png|   Turquía|     2| indice_bajo|\n",
      "|       Chelsea|       chelsea.png|Inglaterra|     1| indice_bajo|\n",
      "|   Club Brugge|   club_brugge.png|   Bélgica|    26| indice_alto|\n",
      "|      Dortmund|      dortmund.png|  Alemania|    27| indice_alto|\n",
      "|   Dynamo Kyiv|   dynamo_kyiv.png|   Ucrania|    28| indice_alto|\n",
      "|Internazionale|internazionale.png|    Italia|    29| indice_alto|\n",
      "|      Juventus|      juventus.png|    Italia|     9| indice_bajo|\n",
      "|       Leipzig|       leipzig.png|  Alemania|     8| indice_bajo|\n",
      "|     Liverpool|     liverpool.png|Inglaterra|    24| indice_alto|\n",
      "|          LOSC|          losc.png|   Francia|    23| indice_alto|\n",
      "|         Malmö|         malmo.png|    Suecia|    22| indice_alto|\n",
      "|     Man. City|      man_city.png|Inglaterra|    21| indice_alto|\n",
      "|   Man. United|    man_united.png|Inglaterra|    20| indice_alto|\n",
      "|         Milan|         milan.png|    Italia|    19|indice_medio|\n",
      "|         Paris|         paris.png|   Francia|    14|indice_medio|\n",
      "+--------------+------------------+----------+------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf1= df1.selectExpr(\"*\", cond)\n",
    "newdf1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be466b8-05cd-4897-9b0e-b916e94634e5",
   "metadata": {},
   "source": [
    "### Funciones definidas por el usuario (UDF)\n",
    "A menudo necesitamos escribir la función en función de nuestro requisito muy específico. Aquí podemos aprovechar las udfs. Podemos escribir nuestras propias funciones en un lenguaje como python y registrar la función como udf, luego podemos usar la función para operaciones de DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "039325e0-26ac-4499-8ca5-b4603c38db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detIndice_Level(ind):\n",
    "    level = None\n",
    "\n",
    "    if(ind <= 10):\n",
    "        level = 'indice_bajo'\n",
    "    elif(ind <20):\n",
    "        level = 'indice_medio'\n",
    "    elif(ind >= 20):\n",
    "        level = 'indice_alto'\n",
    "    else:\n",
    "        level = 'invalid_indice'\n",
    "    return level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7407b113-848a-45d4-9e7c-e6d5888314c2",
   "metadata": {},
   "source": [
    "* Luego registre la función \"detIndice_Level\" como UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7bafe7b7-350a-4ac4-a83e-04a61b5785e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_level = udf(detIndice_Level, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab00ccc-f813-424f-bb34-c7b2873003dc",
   "metadata": {},
   "source": [
    "* Aplicar función para determinar el indice_level para un indice dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d75ca415-799e-40bb-95a3-48ba05e9ef7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+----------+------+------------+\n",
      "|        Equipo|            Imagen|      Pais|Indice|indice_level|\n",
      "+--------------+------------------+----------+------+------------+\n",
      "|      Atalanta|      atalanta.png|    Italia|     6| indice_bajo|\n",
      "|      Atlético|      atletico.png|    España|    11|indice_medio|\n",
      "|     Barcelona|     barcelona.png|    España|    17|indice_medio|\n",
      "|        Bayern|        bayern.png|  Alemania|    16|indice_medio|\n",
      "|       Benfica|       benfica.png|  Portugal|    15|indice_medio|\n",
      "|      Besiktas|      besiktas.png|   Turquía|     2| indice_bajo|\n",
      "|       Chelsea|       chelsea.png|Inglaterra|     1| indice_bajo|\n",
      "|   Club Brugge|   club_brugge.png|   Bélgica|    26| indice_alto|\n",
      "|      Dortmund|      dortmund.png|  Alemania|    27| indice_alto|\n",
      "|   Dynamo Kyiv|   dynamo_kyiv.png|   Ucrania|    28| indice_alto|\n",
      "|Internazionale|internazionale.png|    Italia|    29| indice_alto|\n",
      "|      Juventus|      juventus.png|    Italia|     9| indice_bajo|\n",
      "|       Leipzig|       leipzig.png|  Alemania|     8| indice_bajo|\n",
      "|     Liverpool|     liverpool.png|Inglaterra|    24| indice_alto|\n",
      "|          LOSC|          losc.png|   Francia|    23| indice_alto|\n",
      "|         Malmö|         malmo.png|    Suecia|    22| indice_alto|\n",
      "|     Man. City|      man_city.png|Inglaterra|    21| indice_alto|\n",
      "|   Man. United|    man_united.png|Inglaterra|    20| indice_alto|\n",
      "|         Milan|         milan.png|    Italia|    19|indice_medio|\n",
      "|         Paris|         paris.png|   Francia|    14|indice_medio|\n",
      "+--------------+------------------+----------+------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf1 = df1.withColumn(\"indice_level\", ind_level(\"Indice\"))\n",
    "newdf1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459deff0-0c8e-4058-8e2a-25b62546a600",
   "metadata": {},
   "source": [
    "### Trabajando con valores NULL\n",
    "\n",
    "Los valores NULL siempre son difíciles de manejar independientemente del Framework o lenguaje que usemos. Aquí en Spark tenemos pocas funciones específicas para lidiar con valores NULL.\n",
    "\n",
    "- **es nulo()**\n",
    "\n",
    "Esta función nos ayudará a encontrar los valores nulos para cualquier columna dada. Por ejemplo si necesitamos encontrar las columnas donde las columnas id contienen los valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6c353c80-f3b2-46be-9d30-eb67a0cd2629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----+------+\n",
      "|Equipo|Imagen|Pais|Indice|\n",
      "+------+------+----+------+\n",
      "+------+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf1 = df1.filter(df1[\"Pais\"].isNull())\n",
    "newdf1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d278124-35f2-4fb4-a30d-e88086e8e133",
   "metadata": {},
   "source": [
    "* **No es nulo()**\n",
    "\n",
    "Esta función funciona de manera opuesta a la función isNull () y devolverá todos los valores no nulos para una función en particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8e43a63a-94a9-466c-b10b-f19eabc86609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+----------+------+\n",
      "|        Equipo|            Imagen|      Pais|Indice|\n",
      "+--------------+------------------+----------+------+\n",
      "|      Atalanta|      atalanta.png|    Italia|     6|\n",
      "|      Atlético|      atletico.png|    España|    11|\n",
      "|     Barcelona|     barcelona.png|    España|    17|\n",
      "|        Bayern|        bayern.png|  Alemania|    16|\n",
      "|       Benfica|       benfica.png|  Portugal|    15|\n",
      "|      Besiktas|      besiktas.png|   Turquía|     2|\n",
      "|       Chelsea|       chelsea.png|Inglaterra|     1|\n",
      "|   Club Brugge|   club_brugge.png|   Bélgica|    26|\n",
      "|      Dortmund|      dortmund.png|  Alemania|    27|\n",
      "|   Dynamo Kyiv|   dynamo_kyiv.png|   Ucrania|    28|\n",
      "|Internazionale|internazionale.png|    Italia|    29|\n",
      "|      Juventus|      juventus.png|    Italia|     9|\n",
      "|       Leipzig|       leipzig.png|  Alemania|     8|\n",
      "|     Liverpool|     liverpool.png|Inglaterra|    24|\n",
      "|          LOSC|          losc.png|   Francia|    23|\n",
      "|         Malmö|         malmo.png|    Suecia|    22|\n",
      "|     Man. City|      man_city.png|Inglaterra|    21|\n",
      "|   Man. United|    man_united.png|Inglaterra|    20|\n",
      "|         Milan|         milan.png|    Italia|    19|\n",
      "|         Paris|         paris.png|   Francia|    14|\n",
      "+--------------+------------------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf1 = df1.filter(df1[\"Pais\"].isNotNull())\n",
    "newdf1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b7f5fb-17bc-4ec3-891d-769906a7f1ed",
   "metadata": {},
   "source": [
    "No necesitamos reemplazar valores nulos ni eliminar las filas con valores nulos porque directamente no tenemos valores nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7591f81-3eba-4dd9-940b-4f1c89534a58",
   "metadata": {},
   "source": [
    "## Partitioning\n",
    "\n",
    "\n",
    "El particionamiento es un aspecto muy importante para controlar el paralelismo de la aplicación Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e40f423c-e594-4252-bf8d-628db24ff2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bfcea448-167f-43fe-90cc-b3b3e0eabf42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4941235b-8145-48f6-a96a-8633482cb088",
   "metadata": {},
   "source": [
    "* Incrementar el número de particiones. Por ejemplo Aumentar las particiones a 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0c6b0fcf-3a4c-4cb1-831e-bda72266dc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf1 = df1.repartition(10)\n",
    "newdf1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "47f4d949-7bbd-4659-ab3a-b04c75b898e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf2 = df2.repartition(10)\n",
    "newdf2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf590029-039b-463c-9f8a-3c394973f88c",
   "metadata": {},
   "source": [
    "**Nota: se trata de operaciones costosas, ya que requiere la mezcla de datos entre los trabajadores.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6217aa32-1085-41cb-abd8-a7c53263cb44",
   "metadata": {},
   "source": [
    "* Disminuir el número de particiones. Por ejemplo disminuir las particiones a 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8b30bc49-9409-4d18-b987-eaa092bbf860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf1 = df1.coalesce(4)\n",
    "newdf1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c2e69-a954-4f0c-9f88-4eeb366e6eff",
   "metadata": {},
   "source": [
    "* De forma predeterminada, el número de particiones para Spark SQL es 200.\n",
    "* Pero también podemos establecer el número de particiones en el nivel de aplicación Spark. Por ejemplo establecido en 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "18f18ec3-b448-4062-aab6-dd655ff226fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Partitions : 500\n"
     ]
    }
   ],
   "source": [
    "# Set number of partitions as Spark Application.\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
    "\n",
    "# Check the number of patitions.\n",
    "num_part = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(\"No of Partitions : {0}\".format(num_part))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42cbe75-1fdd-425a-88a5-12c583fbc6c4",
   "metadata": {},
   "source": [
    "# Catálogo de APIs\n",
    "\n",
    "Spark Catalog es una API orientada al usuario, a la que puede acceder mediante SparkSession.catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540c258d-959a-4604-9768-09dd00330fcf",
   "metadata": {},
   "source": [
    "* **listDatabases ()**\n",
    "\n",
    "Devolverá todas las bases de datos junto con su ubicación en el sistema de archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2673a396-5915-49e1-bfec-9064ad094f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/Users/sara/Desktop/SARA/UAX/3º%20CURSO/Segundo%20cuatrimestre/Inteligencia%20artificial/ApacheSpark/spark-warehouse')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f91fbc2-4e66-4244-b8c2-a54e3d05ddb7",
   "metadata": {},
   "source": [
    "* **listTables ()**\n",
    "\n",
    "Devolverá todas las tablas para una base de datos determinada junto con información como el tipo de tabla (externa / administrada) y si una tabla en particular es temporal o permanente.\n",
    "Esto incluye todas las vistas temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9ccc6d40-5458-4eea-9c52-5126e68e282a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='hive_equiposdf', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='hive_partidosdf', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='partidosexterna', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='equipdf', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='partidf', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='temp_table1', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='temp_table2', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa5a920-57a9-4c4c-acf7-09ccf659895c",
   "metadata": {},
   "source": [
    "* **listColumns ()**\n",
    "\n",
    "Devolverá todas las columnas de una tabla en particular en DataBase. Además, devolverá el tipo de datos, si la columna se usa en particiones o agrupaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "377d0495-8bdb-4b97-8793-0b9fef49fac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='Equipo', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='Imagen', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='Pais', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='Indice', description=None, dataType='bigint', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns(\"hive_equiposdf\", \"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416a736c-df44-4586-af0c-99c84f3e91f4",
   "metadata": {},
   "source": [
    "* **listFunctions()**\n",
    "\n",
    "Devolverá todas las funciones disponibles en Spark Session junto con la información si es temporal o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9061b410-9243-4b84-9c2a-f087ea5098d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Function(name='!', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
       " Function(name='%', description=None, className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True),\n",
       " Function(name='&', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseAnd', isTemporary=True),\n",
       " Function(name='*', description=None, className='org.apache.spark.sql.catalyst.expressions.Multiply', isTemporary=True),\n",
       " Function(name='+', description=None, className='org.apache.spark.sql.catalyst.expressions.Add', isTemporary=True),\n",
       " Function(name='-', description=None, className='org.apache.spark.sql.catalyst.expressions.Subtract', isTemporary=True),\n",
       " Function(name='/', description=None, className='org.apache.spark.sql.catalyst.expressions.Divide', isTemporary=True),\n",
       " Function(name='<', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThan', isTemporary=True),\n",
       " Function(name='<=', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThanOrEqual', isTemporary=True),\n",
       " Function(name='<=>', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualNullSafe', isTemporary=True),\n",
       " Function(name='=', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
       " Function(name='==', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
       " Function(name='>', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThan', isTemporary=True),\n",
       " Function(name='>=', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThanOrEqual', isTemporary=True),\n",
       " Function(name='^', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseXor', isTemporary=True),\n",
       " Function(name='abs', description=None, className='org.apache.spark.sql.catalyst.expressions.Abs', isTemporary=True),\n",
       " Function(name='acos', description=None, className='org.apache.spark.sql.catalyst.expressions.Acos', isTemporary=True),\n",
       " Function(name='acosh', description=None, className='org.apache.spark.sql.catalyst.expressions.Acosh', isTemporary=True),\n",
       " Function(name='add_months', description=None, className='org.apache.spark.sql.catalyst.expressions.AddMonths', isTemporary=True),\n",
       " Function(name='aggregate', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayAggregate', isTemporary=True),\n",
       " Function(name='and', description=None, className='org.apache.spark.sql.catalyst.expressions.And', isTemporary=True),\n",
       " Function(name='any', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
       " Function(name='approx_count_distinct', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.HyperLogLogPlusPlus', isTemporary=True),\n",
       " Function(name='approx_percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
       " Function(name='array', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateArray', isTemporary=True),\n",
       " Function(name='array_contains', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayContains', isTemporary=True),\n",
       " Function(name='array_distinct', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayDistinct', isTemporary=True),\n",
       " Function(name='array_except', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayExcept', isTemporary=True),\n",
       " Function(name='array_intersect', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayIntersect', isTemporary=True),\n",
       " Function(name='array_join', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayJoin', isTemporary=True),\n",
       " Function(name='array_max', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayMax', isTemporary=True),\n",
       " Function(name='array_min', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayMin', isTemporary=True),\n",
       " Function(name='array_position', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayPosition', isTemporary=True),\n",
       " Function(name='array_remove', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayRemove', isTemporary=True),\n",
       " Function(name='array_repeat', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayRepeat', isTemporary=True),\n",
       " Function(name='array_sort', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraySort', isTemporary=True),\n",
       " Function(name='array_union', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayUnion', isTemporary=True),\n",
       " Function(name='arrays_overlap', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraysOverlap', isTemporary=True),\n",
       " Function(name='arrays_zip', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraysZip', isTemporary=True),\n",
       " Function(name='ascii', description=None, className='org.apache.spark.sql.catalyst.expressions.Ascii', isTemporary=True),\n",
       " Function(name='asin', description=None, className='org.apache.spark.sql.catalyst.expressions.Asin', isTemporary=True),\n",
       " Function(name='asinh', description=None, className='org.apache.spark.sql.catalyst.expressions.Asinh', isTemporary=True),\n",
       " Function(name='assert_true', description=None, className='org.apache.spark.sql.catalyst.expressions.AssertTrue', isTemporary=True),\n",
       " Function(name='atan', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan', isTemporary=True),\n",
       " Function(name='atan2', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan2', isTemporary=True),\n",
       " Function(name='atanh', description=None, className='org.apache.spark.sql.catalyst.expressions.Atanh', isTemporary=True),\n",
       " Function(name='avg', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
       " Function(name='base64', description=None, className='org.apache.spark.sql.catalyst.expressions.Base64', isTemporary=True),\n",
       " Function(name='bigint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='bin', description=None, className='org.apache.spark.sql.catalyst.expressions.Bin', isTemporary=True),\n",
       " Function(name='binary', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='bit_and', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitAndAgg', isTemporary=True),\n",
       " Function(name='bit_count', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseCount', isTemporary=True),\n",
       " Function(name='bit_get', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseGet', isTemporary=True),\n",
       " Function(name='bit_length', description=None, className='org.apache.spark.sql.catalyst.expressions.BitLength', isTemporary=True),\n",
       " Function(name='bit_or', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitOrAgg', isTemporary=True),\n",
       " Function(name='bit_xor', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitXorAgg', isTemporary=True),\n",
       " Function(name='bool_and', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolAnd', isTemporary=True),\n",
       " Function(name='bool_or', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
       " Function(name='boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='bround', description=None, className='org.apache.spark.sql.catalyst.expressions.BRound', isTemporary=True),\n",
       " Function(name='btrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimBoth', isTemporary=True),\n",
       " Function(name='cardinality', description=None, className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True),\n",
       " Function(name='cast', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='cbrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Cbrt', isTemporary=True),\n",
       " Function(name='ceil', description=None, className='org.apache.spark.sql.catalyst.expressions.Ceil', isTemporary=True),\n",
       " Function(name='ceiling', description=None, className='org.apache.spark.sql.catalyst.expressions.Ceil', isTemporary=True),\n",
       " Function(name='char', description=None, className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True),\n",
       " Function(name='char_length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
       " Function(name='character_length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
       " Function(name='chr', description=None, className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True),\n",
       " Function(name='coalesce', description=None, className='org.apache.spark.sql.catalyst.expressions.Coalesce', isTemporary=True),\n",
       " Function(name='collect_list', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectList', isTemporary=True),\n",
       " Function(name='collect_set', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectSet', isTemporary=True),\n",
       " Function(name='concat', description=None, className='org.apache.spark.sql.catalyst.expressions.Concat', isTemporary=True),\n",
       " Function(name='concat_ws', description=None, className='org.apache.spark.sql.catalyst.expressions.ConcatWs', isTemporary=True),\n",
       " Function(name='conv', description=None, className='org.apache.spark.sql.catalyst.expressions.Conv', isTemporary=True),\n",
       " Function(name='corr', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Corr', isTemporary=True),\n",
       " Function(name='cos', description=None, className='org.apache.spark.sql.catalyst.expressions.Cos', isTemporary=True),\n",
       " Function(name='cosh', description=None, className='org.apache.spark.sql.catalyst.expressions.Cosh', isTemporary=True),\n",
       " Function(name='cot', description=None, className='org.apache.spark.sql.catalyst.expressions.Cot', isTemporary=True),\n",
       " Function(name='count', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Count', isTemporary=True),\n",
       " Function(name='count_if', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CountIf', isTemporary=True),\n",
       " Function(name='count_min_sketch', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CountMinSketchAgg', isTemporary=True),\n",
       " Function(name='covar_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovPopulation', isTemporary=True),\n",
       " Function(name='covar_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovSample', isTemporary=True),\n",
       " Function(name='crc32', description=None, className='org.apache.spark.sql.catalyst.expressions.Crc32', isTemporary=True),\n",
       " Function(name='cume_dist', description=None, className='org.apache.spark.sql.catalyst.expressions.CumeDist', isTemporary=True),\n",
       " Function(name='current_catalog', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentCatalog', isTemporary=True),\n",
       " Function(name='current_database', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDatabase', isTemporary=True),\n",
       " Function(name='current_date', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDate', isTemporary=True),\n",
       " Function(name='current_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimestamp', isTemporary=True),\n",
       " Function(name='current_timezone', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimeZone', isTemporary=True),\n",
       " Function(name='current_user', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentUser', isTemporary=True),\n",
       " Function(name='date', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='date_add', description=None, className='org.apache.spark.sql.catalyst.expressions.DateAdd', isTemporary=True),\n",
       " Function(name='date_format', description=None, className='org.apache.spark.sql.catalyst.expressions.DateFormatClass', isTemporary=True),\n",
       " Function(name='date_from_unix_date', description=None, className='org.apache.spark.sql.catalyst.expressions.DateFromUnixDate', isTemporary=True),\n",
       " Function(name='date_part', description=None, className='org.apache.spark.sql.catalyst.expressions.DatePart', isTemporary=True),\n",
       " Function(name='date_sub', description=None, className='org.apache.spark.sql.catalyst.expressions.DateSub', isTemporary=True),\n",
       " Function(name='date_trunc', description=None, className='org.apache.spark.sql.catalyst.expressions.TruncTimestamp', isTemporary=True),\n",
       " Function(name='datediff', description=None, className='org.apache.spark.sql.catalyst.expressions.DateDiff', isTemporary=True),\n",
       " Function(name='day', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
       " Function(name='dayofmonth', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
       " Function(name='dayofweek', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfWeek', isTemporary=True),\n",
       " Function(name='dayofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfYear', isTemporary=True),\n",
       " Function(name='decimal', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='decode', description=None, className='org.apache.spark.sql.catalyst.expressions.Decode', isTemporary=True),\n",
       " Function(name='degrees', description=None, className='org.apache.spark.sql.catalyst.expressions.ToDegrees', isTemporary=True),\n",
       " Function(name='dense_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.DenseRank', isTemporary=True),\n",
       " Function(name='div', description=None, className='org.apache.spark.sql.catalyst.expressions.IntegralDivide', isTemporary=True),\n",
       " Function(name='double', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='e', description=None, className='org.apache.spark.sql.catalyst.expressions.EulerNumber', isTemporary=True),\n",
       " Function(name='element_at', description=None, className='org.apache.spark.sql.catalyst.expressions.ElementAt', isTemporary=True),\n",
       " Function(name='elt', description=None, className='org.apache.spark.sql.catalyst.expressions.Elt', isTemporary=True),\n",
       " Function(name='encode', description=None, className='org.apache.spark.sql.catalyst.expressions.Encode', isTemporary=True),\n",
       " Function(name='every', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolAnd', isTemporary=True),\n",
       " Function(name='exists', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayExists', isTemporary=True),\n",
       " Function(name='exp', description=None, className='org.apache.spark.sql.catalyst.expressions.Exp', isTemporary=True),\n",
       " Function(name='explode', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True),\n",
       " Function(name='explode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True),\n",
       " Function(name='expm1', description=None, className='org.apache.spark.sql.catalyst.expressions.Expm1', isTemporary=True),\n",
       " Function(name='extract', description=None, className='org.apache.spark.sql.catalyst.expressions.Extract', isTemporary=True),\n",
       " Function(name='factorial', description=None, className='org.apache.spark.sql.catalyst.expressions.Factorial', isTemporary=True),\n",
       " Function(name='filter', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayFilter', isTemporary=True),\n",
       " Function(name='find_in_set', description=None, className='org.apache.spark.sql.catalyst.expressions.FindInSet', isTemporary=True),\n",
       " Function(name='first', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
       " Function(name='first_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
       " Function(name='flatten', description=None, className='org.apache.spark.sql.catalyst.expressions.Flatten', isTemporary=True),\n",
       " Function(name='float', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='floor', description=None, className='org.apache.spark.sql.catalyst.expressions.Floor', isTemporary=True),\n",
       " Function(name='forall', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayForAll', isTemporary=True),\n",
       " Function(name='format_number', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatNumber', isTemporary=True),\n",
       " Function(name='format_string', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
       " Function(name='from_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.CsvToStructs', isTemporary=True),\n",
       " Function(name='from_json', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonToStructs', isTemporary=True),\n",
       " Function(name='from_unixtime', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUnixTime', isTemporary=True),\n",
       " Function(name='from_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUTCTimestamp', isTemporary=True),\n",
       " Function(name='get_json_object', description=None, className='org.apache.spark.sql.catalyst.expressions.GetJsonObject', isTemporary=True),\n",
       " Function(name='getbit', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseGet', isTemporary=True),\n",
       " Function(name='greatest', description=None, className='org.apache.spark.sql.catalyst.expressions.Greatest', isTemporary=True),\n",
       " Function(name='grouping', description=None, className='org.apache.spark.sql.catalyst.expressions.Grouping', isTemporary=True),\n",
       " Function(name='grouping_id', description=None, className='org.apache.spark.sql.catalyst.expressions.GroupingID', isTemporary=True),\n",
       " Function(name='hash', description=None, className='org.apache.spark.sql.catalyst.expressions.Murmur3Hash', isTemporary=True),\n",
       " Function(name='hex', description=None, className='org.apache.spark.sql.catalyst.expressions.Hex', isTemporary=True),\n",
       " Function(name='hour', description=None, className='org.apache.spark.sql.catalyst.expressions.Hour', isTemporary=True),\n",
       " Function(name='hypot', description=None, className='org.apache.spark.sql.catalyst.expressions.Hypot', isTemporary=True),\n",
       " Function(name='if', description=None, className='org.apache.spark.sql.catalyst.expressions.If', isTemporary=True),\n",
       " Function(name='ifnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IfNull', isTemporary=True),\n",
       " Function(name='in', description=None, className='org.apache.spark.sql.catalyst.expressions.In', isTemporary=True),\n",
       " Function(name='initcap', description=None, className='org.apache.spark.sql.catalyst.expressions.InitCap', isTemporary=True),\n",
       " Function(name='inline', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
       " Function(name='inline_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
       " Function(name='input_file_block_length', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockLength', isTemporary=True),\n",
       " Function(name='input_file_block_start', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockStart', isTemporary=True),\n",
       " Function(name='input_file_name', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileName', isTemporary=True),\n",
       " Function(name='instr', description=None, className='org.apache.spark.sql.catalyst.expressions.StringInstr', isTemporary=True),\n",
       " Function(name='int', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='isnan', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNaN', isTemporary=True),\n",
       " Function(name='isnotnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNotNull', isTemporary=True),\n",
       " Function(name='isnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNull', isTemporary=True),\n",
       " Function(name='java_method', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
       " Function(name='json_array_length', description=None, className='org.apache.spark.sql.catalyst.expressions.LengthOfJsonArray', isTemporary=True),\n",
       " Function(name='json_object_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonObjectKeys', isTemporary=True),\n",
       " Function(name='json_tuple', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonTuple', isTemporary=True),\n",
       " Function(name='kurtosis', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Kurtosis', isTemporary=True),\n",
       " Function(name='lag', description=None, className='org.apache.spark.sql.catalyst.expressions.Lag', isTemporary=True),\n",
       " Function(name='last', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
       " Function(name='last_day', description=None, className='org.apache.spark.sql.catalyst.expressions.LastDay', isTemporary=True),\n",
       " Function(name='last_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
       " Function(name='lcase', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
       " Function(name='lead', description=None, className='org.apache.spark.sql.catalyst.expressions.Lead', isTemporary=True),\n",
       " Function(name='least', description=None, className='org.apache.spark.sql.catalyst.expressions.Least', isTemporary=True),\n",
       " Function(name='left', description=None, className='org.apache.spark.sql.catalyst.expressions.Left', isTemporary=True),\n",
       " Function(name='length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
       " Function(name='levenshtein', description=None, className='org.apache.spark.sql.catalyst.expressions.Levenshtein', isTemporary=True),\n",
       " Function(name='like', description=None, className='org.apache.spark.sql.catalyst.expressions.Like', isTemporary=True),\n",
       " Function(name='ln', description=None, className='org.apache.spark.sql.catalyst.expressions.Log', isTemporary=True),\n",
       " Function(name='locate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True),\n",
       " Function(name='log', description=None, className='org.apache.spark.sql.catalyst.expressions.Logarithm', isTemporary=True),\n",
       " Function(name='log10', description=None, className='org.apache.spark.sql.catalyst.expressions.Log10', isTemporary=True),\n",
       " Function(name='log1p', description=None, className='org.apache.spark.sql.catalyst.expressions.Log1p', isTemporary=True),\n",
       " Function(name='log2', description=None, className='org.apache.spark.sql.catalyst.expressions.Log2', isTemporary=True),\n",
       " Function(name='lower', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
       " Function(name='lpad', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLPad', isTemporary=True),\n",
       " Function(name='ltrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimLeft', isTemporary=True),\n",
       " Function(name='make_date', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeDate', isTemporary=True),\n",
       " Function(name='make_dt_interval', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeDTInterval', isTemporary=True),\n",
       " Function(name='make_interval', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeInterval', isTemporary=True),\n",
       " Function(name='make_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeTimestamp', isTemporary=True),\n",
       " Function(name='make_ym_interval', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeYMInterval', isTemporary=True),\n",
       " Function(name='map', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateMap', isTemporary=True),\n",
       " Function(name='map_concat', description=None, className='org.apache.spark.sql.catalyst.expressions.MapConcat', isTemporary=True),\n",
       " Function(name='map_entries', description=None, className='org.apache.spark.sql.catalyst.expressions.MapEntries', isTemporary=True),\n",
       " Function(name='map_filter', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFilter', isTemporary=True),\n",
       " Function(name='map_from_arrays', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFromArrays', isTemporary=True),\n",
       " Function(name='map_from_entries', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFromEntries', isTemporary=True),\n",
       " Function(name='map_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.MapKeys', isTemporary=True),\n",
       " Function(name='map_values', description=None, className='org.apache.spark.sql.catalyst.expressions.MapValues', isTemporary=True),\n",
       " Function(name='map_zip_with', description=None, className='org.apache.spark.sql.catalyst.expressions.MapZipWith', isTemporary=True),\n",
       " Function(name='max', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Max', isTemporary=True),\n",
       " Function(name='max_by', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.MaxBy', isTemporary=True),\n",
       " Function(name='md5', description=None, className='org.apache.spark.sql.catalyst.expressions.Md5', isTemporary=True),\n",
       " Function(name='mean', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
       " Function(name='min', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Min', isTemporary=True),\n",
       " Function(name='min_by', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.MinBy', isTemporary=True),\n",
       " Function(name='minute', description=None, className='org.apache.spark.sql.catalyst.expressions.Minute', isTemporary=True),\n",
       " Function(name='mod', description=None, className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True),\n",
       " Function(name='monotonically_increasing_id', description=None, className='org.apache.spark.sql.catalyst.expressions.MonotonicallyIncreasingID', isTemporary=True),\n",
       " Function(name='month', description=None, className='org.apache.spark.sql.catalyst.expressions.Month', isTemporary=True),\n",
       " Function(name='months_between', description=None, className='org.apache.spark.sql.catalyst.expressions.MonthsBetween', isTemporary=True),\n",
       " Function(name='named_struct', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True),\n",
       " Function(name='nanvl', description=None, className='org.apache.spark.sql.catalyst.expressions.NaNvl', isTemporary=True),\n",
       " Function(name='negative', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryMinus', isTemporary=True),\n",
       " Function(name='next_day', description=None, className='org.apache.spark.sql.catalyst.expressions.NextDay', isTemporary=True),\n",
       " Function(name='not', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
       " Function(name='now', description=None, className='org.apache.spark.sql.catalyst.expressions.Now', isTemporary=True),\n",
       " Function(name='nth_value', description=None, className='org.apache.spark.sql.catalyst.expressions.NthValue', isTemporary=True),\n",
       " Function(name='ntile', description=None, className='org.apache.spark.sql.catalyst.expressions.NTile', isTemporary=True),\n",
       " Function(name='nullif', description=None, className='org.apache.spark.sql.catalyst.expressions.NullIf', isTemporary=True),\n",
       " Function(name='nvl', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl', isTemporary=True),\n",
       " Function(name='nvl2', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl2', isTemporary=True),\n",
       " Function(name='octet_length', description=None, className='org.apache.spark.sql.catalyst.expressions.OctetLength', isTemporary=True),\n",
       " Function(name='or', description=None, className='org.apache.spark.sql.catalyst.expressions.Or', isTemporary=True),\n",
       " Function(name='overlay', description=None, className='org.apache.spark.sql.catalyst.expressions.Overlay', isTemporary=True),\n",
       " Function(name='parse_url', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseUrl', isTemporary=True),\n",
       " Function(name='percent_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.PercentRank', isTemporary=True),\n",
       " Function(name='percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Percentile', isTemporary=True),\n",
       " Function(name='percentile_approx', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
       " Function(name='pi', description=None, className='org.apache.spark.sql.catalyst.expressions.Pi', isTemporary=True),\n",
       " Function(name='pmod', description=None, className='org.apache.spark.sql.catalyst.expressions.Pmod', isTemporary=True),\n",
       " Function(name='posexplode', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
       " Function(name='posexplode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
       " Function(name='position', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True),\n",
       " Function(name='positive', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryPositive', isTemporary=True),\n",
       " Function(name='pow', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
       " Function(name='power', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
       " Function(name='printf', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
       " Function(name='quarter', description=None, className='org.apache.spark.sql.catalyst.expressions.Quarter', isTemporary=True),\n",
       " Function(name='radians', description=None, className='org.apache.spark.sql.catalyst.expressions.ToRadians', isTemporary=True),\n",
       " Function(name='raise_error', description=None, className='org.apache.spark.sql.catalyst.expressions.RaiseError', isTemporary=True),\n",
       " Function(name='rand', description=None, className='org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True),\n",
       " Function(name='randn', description=None, className='org.apache.spark.sql.catalyst.expressions.Randn', isTemporary=True),\n",
       " Function(name='random', description=None, className='org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True),\n",
       " Function(name='range', description=None, className='org.apache.spark.sql.catalyst.plans.logical.Range', isTemporary=True),\n",
       " Function(name='rank', description=None, className='org.apache.spark.sql.catalyst.expressions.Rank', isTemporary=True),\n",
       " Function(name='reflect', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
       " Function(name='regexp', description=None, className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True),\n",
       " Function(name='regexp_extract', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpExtract', isTemporary=True),\n",
       " Function(name='regexp_extract_all', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpExtractAll', isTemporary=True),\n",
       " Function(name='regexp_like', description=None, className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True),\n",
       " Function(name='regexp_replace', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpReplace', isTemporary=True),\n",
       " Function(name='repeat', description=None, className='org.apache.spark.sql.catalyst.expressions.StringRepeat', isTemporary=True),\n",
       " Function(name='replace', description=None, className='org.apache.spark.sql.catalyst.expressions.StringReplace', isTemporary=True),\n",
       " Function(name='reverse', description=None, className='org.apache.spark.sql.catalyst.expressions.Reverse', isTemporary=True),\n",
       " Function(name='right', description=None, className='org.apache.spark.sql.catalyst.expressions.Right', isTemporary=True),\n",
       " Function(name='rint', description=None, className='org.apache.spark.sql.catalyst.expressions.Rint', isTemporary=True),\n",
       " Function(name='rlike', description=None, className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True),\n",
       " Function(name='round', description=None, className='org.apache.spark.sql.catalyst.expressions.Round', isTemporary=True),\n",
       " Function(name='row_number', description=None, className='org.apache.spark.sql.catalyst.expressions.RowNumber', isTemporary=True),\n",
       " Function(name='rpad', description=None, className='org.apache.spark.sql.catalyst.expressions.StringRPad', isTemporary=True),\n",
       " Function(name='rtrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimRight', isTemporary=True),\n",
       " Function(name='schema_of_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.SchemaOfCsv', isTemporary=True),\n",
       " Function(name='schema_of_json', description=None, className='org.apache.spark.sql.catalyst.expressions.SchemaOfJson', isTemporary=True),\n",
       " Function(name='second', description=None, className='org.apache.spark.sql.catalyst.expressions.Second', isTemporary=True),\n",
       " Function(name='sentences', description=None, className='org.apache.spark.sql.catalyst.expressions.Sentences', isTemporary=True),\n",
       " Function(name='sequence', description=None, className='org.apache.spark.sql.catalyst.expressions.Sequence', isTemporary=True),\n",
       " Function(name='session_window', description=None, className='org.apache.spark.sql.catalyst.expressions.SessionWindow', isTemporary=True),\n",
       " Function(name='sha', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
       " Function(name='sha1', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
       " Function(name='sha2', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha2', isTemporary=True),\n",
       " Function(name='shiftleft', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftLeft', isTemporary=True),\n",
       " Function(name='shiftright', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRight', isTemporary=True),\n",
       " Function(name='shiftrightunsigned', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRightUnsigned', isTemporary=True),\n",
       " Function(name='shuffle', description=None, className='org.apache.spark.sql.catalyst.expressions.Shuffle', isTemporary=True),\n",
       " Function(name='sign', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
       " Function(name='signum', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
       " Function(name='sin', description=None, className='org.apache.spark.sql.catalyst.expressions.Sin', isTemporary=True),\n",
       " Function(name='sinh', description=None, className='org.apache.spark.sql.catalyst.expressions.Sinh', isTemporary=True),\n",
       " Function(name='size', description=None, className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True),\n",
       " Function(name='skewness', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Skewness', isTemporary=True),\n",
       " Function(name='slice', description=None, className='org.apache.spark.sql.catalyst.expressions.Slice', isTemporary=True),\n",
       " Function(name='smallint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='some', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
       " Function(name='sort_array', description=None, className='org.apache.spark.sql.catalyst.expressions.SortArray', isTemporary=True),\n",
       " Function(name='soundex', description=None, className='org.apache.spark.sql.catalyst.expressions.SoundEx', isTemporary=True),\n",
       " Function(name='space', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSpace', isTemporary=True),\n",
       " Function(name='spark_partition_id', description=None, className='org.apache.spark.sql.catalyst.expressions.SparkPartitionID', isTemporary=True),\n",
       " Function(name='split', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSplit', isTemporary=True),\n",
       " Function(name='sqrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Sqrt', isTemporary=True),\n",
       " Function(name='stack', description=None, className='org.apache.spark.sql.catalyst.expressions.Stack', isTemporary=True),\n",
       " Function(name='std', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name='stddev', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name='stddev_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevPop', isTemporary=True),\n",
       " Function(name='stddev_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name='str_to_map', description=None, className='org.apache.spark.sql.catalyst.expressions.StringToMap', isTemporary=True),\n",
       " Function(name='string', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='struct', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True),\n",
       " Function(name='substr', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
       " Function(name='substring', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
       " Function(name='substring_index', description=None, className='org.apache.spark.sql.catalyst.expressions.SubstringIndex', isTemporary=True),\n",
       " Function(name='sum', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Sum', isTemporary=True),\n",
       " Function(name='tan', description=None, className='org.apache.spark.sql.catalyst.expressions.Tan', isTemporary=True),\n",
       " Function(name='tanh', description=None, className='org.apache.spark.sql.catalyst.expressions.Tanh', isTemporary=True),\n",
       " Function(name='timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='timestamp_micros', description=None, className='org.apache.spark.sql.catalyst.expressions.MicrosToTimestamp', isTemporary=True),\n",
       " Function(name='timestamp_millis', description=None, className='org.apache.spark.sql.catalyst.expressions.MillisToTimestamp', isTemporary=True),\n",
       " Function(name='timestamp_seconds', description=None, className='org.apache.spark.sql.catalyst.expressions.SecondsToTimestamp', isTemporary=True),\n",
       " Function(name='tinyint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='to_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.StructsToCsv', isTemporary=True),\n",
       " Function(name='to_date', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToDate', isTemporary=True),\n",
       " Function(name='to_json', description=None, className='org.apache.spark.sql.catalyst.expressions.StructsToJson', isTemporary=True),\n",
       " Function(name='to_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToTimestamp', isTemporary=True),\n",
       " Function(name='to_unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUnixTimestamp', isTemporary=True),\n",
       " Function(name='to_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUTCTimestamp', isTemporary=True),\n",
       " Function(name='transform', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayTransform', isTemporary=True),\n",
       " Function(name='transform_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.TransformKeys', isTemporary=True),\n",
       " Function(name='transform_values', description=None, className='org.apache.spark.sql.catalyst.expressions.TransformValues', isTemporary=True),\n",
       " Function(name='translate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTranslate', isTemporary=True),\n",
       " Function(name='trim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrim', isTemporary=True),\n",
       " Function(name='trunc', description=None, className='org.apache.spark.sql.catalyst.expressions.TruncDate', isTemporary=True),\n",
       " Function(name='try_add', description=None, className='org.apache.spark.sql.catalyst.expressions.TryAdd', isTemporary=True),\n",
       " Function(name='try_divide', description=None, className='org.apache.spark.sql.catalyst.expressions.TryDivide', isTemporary=True),\n",
       " Function(name='typeof', description=None, className='org.apache.spark.sql.catalyst.expressions.TypeOf', isTemporary=True),\n",
       " Function(name='ucase', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
       " Function(name='unbase64', description=None, className='org.apache.spark.sql.catalyst.expressions.UnBase64', isTemporary=True),\n",
       " Function(name='unhex', description=None, className='org.apache.spark.sql.catalyst.expressions.Unhex', isTemporary=True),\n",
       " Function(name='unix_date', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixDate', isTemporary=True),\n",
       " Function(name='unix_micros', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixMicros', isTemporary=True),\n",
       " Function(name='unix_millis', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixMillis', isTemporary=True),\n",
       " Function(name='unix_seconds', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixSeconds', isTemporary=True),\n",
       " Function(name='unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixTimestamp', isTemporary=True),\n",
       " Function(name='upper', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
       " Function(name='uuid', description=None, className='org.apache.spark.sql.catalyst.expressions.Uuid', isTemporary=True),\n",
       " Function(name='var_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VariancePop', isTemporary=True),\n",
       " Function(name='var_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
       " Function(name='variance', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
       " Function(name='version', description=None, className='org.apache.spark.sql.catalyst.expressions.SparkVersion', isTemporary=True),\n",
       " Function(name='weekday', description=None, className='org.apache.spark.sql.catalyst.expressions.WeekDay', isTemporary=True),\n",
       " Function(name='weekofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.WeekOfYear', isTemporary=True),\n",
       " Function(name='when', description=None, className='org.apache.spark.sql.catalyst.expressions.CaseWhen', isTemporary=True),\n",
       " Function(name='width_bucket', description=None, className='org.apache.spark.sql.catalyst.expressions.WidthBucket', isTemporary=True),\n",
       " Function(name='window', description=None, className='org.apache.spark.sql.catalyst.expressions.TimeWindow', isTemporary=True),\n",
       " Function(name='xpath', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathList', isTemporary=True),\n",
       " Function(name='xpath_boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathBoolean', isTemporary=True),\n",
       " Function(name='xpath_double', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
       " Function(name='xpath_float', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathFloat', isTemporary=True),\n",
       " Function(name='xpath_int', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathInt', isTemporary=True),\n",
       " Function(name='xpath_long', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathLong', isTemporary=True),\n",
       " Function(name='xpath_number', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
       " Function(name='xpath_short', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathShort', isTemporary=True),\n",
       " Function(name='xpath_string', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathString', isTemporary=True),\n",
       " Function(name='xxhash64', description=None, className='org.apache.spark.sql.catalyst.expressions.XxHash64', isTemporary=True),\n",
       " Function(name='year', description=None, className='org.apache.spark.sql.catalyst.expressions.Year', isTemporary=True),\n",
       " Function(name='zip_with', description=None, className='org.apache.spark.sql.catalyst.expressions.ZipWith', isTemporary=True),\n",
       " Function(name='|', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseOr', isTemporary=True),\n",
       " Function(name='~', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseNot', isTemporary=True)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listFunctions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53dd6ed-08ee-456c-a5d7-8c1ab5ff7607",
   "metadata": {},
   "source": [
    "* **currentDatabase ()**\n",
    "\n",
    "Obtenemos la base de datos actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "dcb9cb2c-6a70-40f2-8793-984aaf3c6374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3d8939-7c27-4ee8-97bf-ea993659586d",
   "metadata": {},
   "source": [
    "* **setCurrentDatabase ()**\n",
    "\n",
    "Establecer la base de datos actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2ae2b2f4-2d8b-4b7f-b8f0-1418c6463ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d56a16-4322-43fd-8d40-fd186175700d",
   "metadata": {},
   "source": [
    "* **cacheTable ()**\n",
    "\n",
    "almacenar en caché una tabla en particular.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "44fa37c0-50a5-43c6-8233-006f5eab9875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/18 12:14:30 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "spark.catalog.cacheTable(\"default.hive_equiposdf\")\n",
    "spark.catalog.cacheTable(\"default.hive_partidosdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b2828-b160-4ef0-a287-8e71d6fcdf32",
   "metadata": {},
   "source": [
    "* **isCached()**\n",
    "\n",
    "Compruebe si la tabla está almacenada en caché o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ef1342d1-c7f1-4541-9793-56035b14bfa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.isCached(\"default.hive_equiposdf\")\n",
    "spark.catalog.isCached(\"default.hive_partidosdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc25e0f-5804-4386-ab68-9dea37d44d68",
   "metadata": {},
   "source": [
    "* **uncacheTable()**\n",
    "\n",
    "Des-cachear de una tabla en particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "454d1611-9217-496d-a1b2-9cf16f50e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.uncacheTable(\"default.hive_equiposdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a0a1f43f-0b92-464a-bf7f-25f9ee8a9e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify uncached table. Now you will see that it will return \"False\" which means table is not cached.\n",
    "spark.catalog.isCached(\"default.hive_equiposdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1732848a-fe2e-43ec-87b6-01f618009b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Esta debería salir \"True\"\n",
    "spark.catalog.isCached(\"default.hive_partidosdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1a9a4c-648c-4068-9bab-cdb791aab872",
   "metadata": {},
   "source": [
    "* **clearCache()**\n",
    "\n",
    "Des-cachear toda la tabla en la sesión de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "01c587fd-8964-40ec-a1f0-e4a1d4320f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70e75ee-e60b-436a-8872-b1b11d4af7fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
